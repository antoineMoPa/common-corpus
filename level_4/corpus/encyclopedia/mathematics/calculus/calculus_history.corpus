The history of calculus represents one of the most profound intellectual achievements in human thought, marking the transition from static mathematics to a dynamic science capable of describing change and motion. While often attributed to the independent work of Isaac Newton and Gottfried Wilhelm Leibniz in the late seventeenth century, the development of calculus was a cumulative process spanning millennia, evolving from intuitive geometric methods to a rigorous logical framework that underpins modern science and engineering.

The origins of calculus can be traced back to ancient civilizations that grappled with problems involving areas, volumes, and tangents. In ancient Egypt and Babylon, mathematicians developed empirical formulas for calculating the volume of granaries and the area of fields, yet these methods lacked a general theoretical basis. It was in ancient Greece that the first systematic approaches emerged. Archimedes of Syracuse, working in the third century BCE, perfected the method of exhaustion, a technique used to determine areas and volumes by inscribing and circumscribing polygons with an increasing number of sides. By proving that the difference between the polygon and the curve could be made smaller than any given quantity, Archimedes effectively utilized a precursor to the limit concept. Similarly, Eudoxus of Cnidus employed similar reasoning to compare curvilinear figures. However, Greek mathematics remained largely geometric, and the lack of algebraic notation prevented these insights from coalescing into a unified algorithm for handling rates of change.

Following the decline of classical antiquity, significant advancements occurred in the Islamic Golden Age and later in India. Mathematicians such as Alhazen calculated the sum of fourth powers to determine the volume of a paraboloid, while Indian scholars of the Kerala school, notably Madhava of Sangamagrama in the fourteenth century, discovered infinite series expansions for trigonometric functions. These series, which included early forms of the Taylor series, demonstrated a sophisticated understanding of infinitesimal processes, though this knowledge did not immediately diffuse to Europe to catalyze a broader synthesis.

The Renaissance and the Scientific Revolution created the necessary conditions for the birth of modern calculus. As astronomers and physicists sought to describe planetary motion and the behavior of falling bodies, existing mathematical tools proved inadequate. Johannes Kepler's laws of planetary motion and Galileo Galilei's studies on acceleration demanded a mathematics of flux and flow. In the early seventeenth century, Bonaventura Cavalieri introduced the method of indivisibles, treating lines as composed of infinite points and areas as composed of infinite lines. Although criticized for its lack of rigor, this heuristic approach allowed for rapid calculation of areas and volumes. Simultaneously, Pierre de Fermat developed methods for finding maxima and minima and constructing tangents to curves, effectively performing differentiation without the benefit of a formal derivative concept.

The culmination of these disparate threads occurred in the latter half of the seventeenth century through the independent work of Isaac Newton and Gottfried Wilhelm Leibniz. Newton, motivated by problems in physics, developed his method of "fluxions" in the 1660s. He viewed variables as flowing quantities, or fluents, and their rates of change as fluxions. His approach was deeply rooted in kinematics and provided powerful tools for solving problems in mechanics and optics. However, Newton was reluctant to publish his findings, leading to a delay in their dissemination.

Conversely, Leibniz, working in the 1670s, approached the subject from a more geometric and philosophical perspective. He focused on the characteristics of the tangent and the area under a curve, recognizing the inverse relationship between differentiation and integration, now known as the Fundamental Theorem of Calculus. Leibniz's greatest contribution lay in his notation; he introduced the symbols $dx$ and $\int$, which elegantly represented infinitesimal differences and sums. This notation was intuitive and flexible, facilitating the manipulation of complex expressions and accelerating the adoption of calculus across continental Europe. The subsequent priority dispute between Newton and Leibniz, fueled by nationalistic fervor, unfortunately divided the mathematical community for a century, with British mathematicians clinging to Newton's fluxional notation while their continental counterparts advanced the field using Leibniz's superior symbolic language.

During the eighteenth century, the Bernoulli family, Leonhard Euler, and Joseph-Louis Lagrange expanded the scope of calculus, applying it to diverse fields such as fluid dynamics, elasticity, and celestial mechanics. Euler, in particular, treated functions as the central object of study and developed the calculus of variations. Yet, despite its practical success, the foundations of calculus remained shaky. The reliance on infinitesimals—quantities that were neither zero nor non-zero—drew sharp criticism, most famously from Bishop George Berkeley, who derided them as "the ghosts of departed quantities." The lack of a rigorous definition for limits and continuity meant that calculus operated on intuitive grounds that occasionally led to paradoxes.

The nineteenth century witnessed a movement toward arithmetization and rigor that transformed calculus into analysis. Augustin-Louis Cauchy was pivotal in this shift, replacing vague notions of infinitesimals with precise definitions of limits, continuity, and convergence. He formalized the derivative as the limit of a difference quotient and the definite integral as the limit of a sum. Later, Karl Weierstrass further refined these concepts by providing the rigorous epsilon-delta definition of the limit, entirely eliminating the need for infinitesimals in the foundational logic. This era also saw Georg Cantor's development of set theory, which provided the necessary framework for understanding the real number line and the nature of continuity. By the end of the century, calculus had been rebuilt on a solid logical foundation, separating the intuitive heuristics of discovery from the strict requirements of proof.

In the twentieth and twenty-first centuries, the evolution of calculus continued along multiple paths. The development of measure theory by Henri Lebesgue generalized the concept of integration, allowing mathematicians to integrate a much broader class of functions than was possible with the Riemann integral. This advancement proved crucial for probability theory and quantum mechanics. Furthermore, the mid-twentieth century saw Abraham Robinson develop non-standard analysis, which finally provided a rigorous logical basis for infinitesimals using model theory, vindicating the intuitive approaches of Leibniz and Cauchy in a new formal context.

Today, calculus remains a vibrant and essential discipline. Its practice has evolved from manual computation to complex numerical simulations powered by computers, enabling the modeling of chaotic systems, financial markets, and climate change. While the core principles established by Newton, Leibniz, and their successors remain unchanged, the depth of understanding and the breadth of application have expanded immensely. From its ancient geometric roots to its modern abstract formulations, the history of calculus illustrates the human capacity to refine intuitive insights into a powerful, rigorous language for describing the dynamics of the universe.
